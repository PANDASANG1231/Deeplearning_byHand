{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "x = torch.arange(12.0, requires_grad=True)\n",
    "x.requires_grad_(True)\n",
    "\n",
    "## 计算第一个函数\n",
    "y = 2 * torch.dot(x, x)\n",
    "print(y.requires_grad, y)\n",
    "## 反向传播\n",
    "y.backward()\n",
    "## 传播玩之后，可以看x的每个导数\n",
    "print(x.grad, x.grad == 4 * x)\n",
    "\n",
    "\n",
    "## 在默认情况下，pytorch会累积梯度，所以要清空一下梯度\n",
    "x.grad.zero_()\n",
    "## 现在来计算另一个函数\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "print(x.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True tensor(1012., grad_fn=<MulBackward0>)\n",
      "tensor([ 0.,  4.,  8., 12., 16., 20., 24., 28., 32., 36., 40., 44.]) tensor([True, True, True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "假设y是一个向量的话，在深度学习中，计算不是为了计算微分矩阵，所以会报错"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "print(y, '\\n')\n",
    "\n",
    "try:\n",
    "    y.backward()\n",
    "except Exception as err:\n",
    "    print(\"Error: \", err)\n",
    "    \n",
    "y = y.sum()\n",
    "y.backward()\n",
    "print(x.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([  0.,   1.,   4.,   9.,  16.,  25.,  36.,  49.,  64.,  81., 100., 121.],\n",
      "       grad_fn=<MulBackward0>) \n",
      "\n",
      "Error:  grad can be implicitly created only for scalar outputs\n",
      "tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20., 22.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "把某些计算移动到记录的计算图以外"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x \n",
    "\n",
    "## z这个时候系统看u是和x无关的常数\n",
    "z.sum().backward()\n",
    "print(x.grad == u)\n",
    "\n",
    "## y还在计算图里面，如果看y.sum(),还是一个高阶项\n",
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "print(x.grad == 2 * x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "即使在使用了python控制流之后，仍可以计算变量自动求导"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "def f(a):\n",
    "    \n",
    "    for i in range(10):\n",
    "        a = a * 2.\n",
    "    \n",
    "    if a.sum() > 1000:\n",
    "        return a\n",
    "    else:\n",
    "        return a/1000\n",
    "\n",
    "x.grad.zero_()\n",
    "y = f(x).sum()\n",
    "y.backward()\n",
    "print(x.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1024., 1024., 1024., 1024., 1024., 1024., 1024., 1024., 1024., 1024.,\n",
      "        1024., 1024.])\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "20ba1d4ab6bbc91ce82013c71f11f6aebee508b4070cd6657dc06b0d0f5dc091"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}