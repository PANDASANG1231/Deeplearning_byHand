{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst = []\n",
    "with open(\"./../data/music.json\") as f:\n",
    "    for i in f.readlines():\n",
    "        i = json.loads(i)\n",
    "        rst.append(i)\n",
    "rst = pd.DataFrame(rst)[[\"singer\", \"geci\", \"album\"]]\n",
    "\n",
    "\n",
    "def preprocess_geci(x):\n",
    "    lines = []\n",
    "    for line in x:\n",
    "        line = line.replace(\",\", \"\") \\\n",
    "                .replace(\".\", \"\") \\\n",
    "                .replace(\"，\", \"\") \\\n",
    "                .replace(\"。\", \"\") \\\n",
    "                .replace(\"《\", \"\") \\\n",
    "                .replace(\"》\", \"\") \n",
    "\n",
    "        lines.append(line)\n",
    "    return \"，\".join(lines)\n",
    "\n",
    "def is_character(x):\n",
    "    res = True\n",
    "    for w in x:\n",
    "        if not '\\u4e00' <= w <= '\\u9fff':\n",
    "            res = False\n",
    "    return res\n",
    "\n",
    "        \n",
    "rst[\"geci\"] = rst[\"geci\"].apply(preprocess_geci)\n",
    "rst[\"album\"] = rst[\"album\"].apply(lambda x:x.replace(\"《\", \"\").replace(\"》\", \"\"))\n",
    "rst[\"mark\"] = rst[\"singer\"].apply(is_character) | rst[\"album\"].apply(is_character)\n",
    "rst = rst.query(\"mark == True\").reset_index(drop=True)\n",
    "\n",
    "corpus = \"\".join(rst[\"geci\"])\n",
    "keys = list(Counter(corpus).keys()) + [' ']\n",
    "\n",
    "num_to_char = {k:v for k,v in enumerate(keys)}\n",
    "char_to_num = {v:k for k,v in enumerate(keys)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import utils\n",
    "from torch import nn\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class musicDataSet(utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, seq_len=3, time_len=100, char_to_num=char_to_num):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.seq_len = seq_len\n",
    "        self.time_len = time_len\n",
    "        self.char_to_num = char_to_num\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataframe.shape[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def __convert_onehot__(list_corpus):\n",
    "        array_corpus = np.array(list_corpus)\n",
    "        rst_array = np.zeros((array_corpus.size, max(char_to_num.values()) + 1))\n",
    "        rst_array[np.arange(array_corpus.size), array_corpus] = 1\n",
    "        return rst_array.flatten()\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        corpus_item = self.dataframe.loc[idx, 'geci']\n",
    "        max_len = len(corpus_item)\n",
    "        X, y = [], []\n",
    "        for i in range(self.time_len):\n",
    "            if i >= max_len - self.seq_len:\n",
    "                tmp_list = []\n",
    "                for num in range(self.seq_len):\n",
    "                    tmp_list.append(char_to_num[\" \"])\n",
    "                    \n",
    "                # X.append(self.__convert_onehot__(tmp_list))\n",
    "                y.append(self.__convert_onehot__(char_to_num[\" \"]))\n",
    "                X.append(tmp_list)\n",
    "                # y.append(char_to_num[\" \"])\n",
    "\n",
    "            else:\n",
    "                tmp_list = []\n",
    "                for num in range(self.seq_len):\n",
    "                    tmp_list.append(char_to_num[corpus_item[i+num]])\n",
    "                    \n",
    "                # X.append(self.__convert_onehot__(tmp_list))\n",
    "                y.append(self.__convert_onehot__(char_to_num[corpus_item[i+self.seq_len]]))\n",
    "                X.append(tmp_list)\n",
    "                # y.append(char_to_num[corpus_item[i+self.seq_len]])\n",
    "\n",
    "        X, y = torch.from_numpy(np.array(X)), torch.from_numpy(np.array(y))\n",
    "        \n",
    "        # return X, y\n",
    "        return X, y\n",
    "    \n",
    "\n",
    "train_data, test_data = train_test_split(rst, train_size=0.9, shuffle=True, random_state=2022)\n",
    "train_dataset = musicDataSet(train_data, seq_len=3, time_len=100)\n",
    "test_dataset = musicDataSet(test_data, seq_len=3, time_len=100)\n",
    "\n",
    "train_dataloader = utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lyricModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, embedding_dim, seq_len, hidden_size=128, num_layers=2):\n",
    "        super(lyricModel, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(\n",
    "                num_embeddings=n_vocab, \n",
    "                embedding_dim=embedding_dim\n",
    "            )\n",
    "        \n",
    "        self.backbone = nn.GRU(\n",
    "                input_size=embedding_dim*seq_len, \n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                dropout=0.3,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        \n",
    "        self.tail = nn.Linear(hidden_size, n_vocab)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        dim1, dim2, _ = x.shape\n",
    "        x = self.embed(x).reshape(dim1, dim2, -1)\n",
    "        x, h = self.backbone(x)\n",
    "        x = self.tail(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "n_vocab = len(char_to_num.keys()) + 1\n",
    "embedding_dim = 128\n",
    "lyric_model = lyricModel(n_vocab=n_vocab, embedding_dim=embedding_dim, seq_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 9273]) torch.Size([100, 32, 9273])\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "    X, y = X.permute(1, 0, 2), y.permute(1, 0, 2)\n",
    "    pred = lyric_model(X)\n",
    "    print(pred.shape, y.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('impact_detection_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb7ace946bd0f4778ebc151d1b7a5ebc3041135d2917aadb5559fba3a354c776"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
